{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24990,
     "status": "ok",
     "timestamp": 1552026972308,
     "user": {
      "displayName": "Koala Xiong",
      "photoUrl": "https://lh3.googleusercontent.com/-LPYBIG6zcRU/AAAAAAAAAAI/AAAAAAAAAgw/cjhI33fxI8A/s64/photo.jpg",
      "userId": "10520705697035462845"
     },
     "user_tz": -480
    },
    "id": "3K89ObMD1wCM",
    "outputId": "12804880-3de7-4121-d054-870f901ed602"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport tensorflow as tf\\nfrom keras import backend as K\\nfrom keras.models import Model, Sequential\\nfrom keras import regularizers, optimizers\\nfrom keras.applications import InceptionV3\\nfrom keras.applications.inception_v3 import preprocess_input\\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\\nfrom keras.layers import Dropout, Flatten, Dense\\nfrom keras.layers import GlobalAveragePooling2D,Conv2D,MaxPooling2D,Activation\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.utils.np_utils import to_categorical\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from matplotlib import pyplot\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.applications import InceptionV3\n",
    "from keras.layers import GlobalAveragePooling2D,Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "#from keras.optimizers import RMSprop\n",
    "#from keras.layers.advanced_activations import LeakyReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oHMDSzMYy805"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nrepo_path = '/content/gdrive/My Drive/Course2/NDSC-advanced_competition'\\ndoc_path = repo_path\\n\\nworking_area = ['fashion', 'beauty','mobile']\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_cores = 1\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                       inter_op_parallelism_threads=1,\n",
    "                       allow_soft_placement=True,\n",
    "                       device_count = {'GPU': 1}\n",
    "                       )\n",
    "\n",
    "#{'GPU': 1,  'CPU': 1}\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iysapo5zKk1z"
   },
   "outputs": [],
   "source": [
    "\n",
    "doc_path = os.path.curdir\n",
    "base_path = '/dataset/NDSC'\n",
    "repo_path = os.path.join('e:',base_path)\n",
    "\n",
    "\n",
    "working_area = ['fashion', 'beauty','mobile']\n",
    "fixbug=False\n",
    "\n",
    "def without_new(old):\n",
    "    return old.replace(\"new_\", \"\")\n",
    "  \n",
    "def is_class(old, classes):\n",
    "    return old+'_'+classes\n",
    "  \n",
    "def strnumcombin(a):\n",
    "    return a[0]+' '+a[1]\n",
    "\n",
    "def data_prepare(working_area, fixbug=False):    \n",
    "    df_train = {}\n",
    "    df_val = {}\n",
    "    df_profile = {}\n",
    "    for work in working_area:\n",
    "        df_train[work] = pd.read_csv(os.path.join(doc_path,work+\"_data_info_train_competition.csv\"))\n",
    "    for work in working_area:\n",
    "        df_val[work] = pd.read_csv(os.path.join(doc_path,work+\"_data_info_val_competition.csv\"))\n",
    "    for work in working_area:\n",
    "        df_profile[work] = pd.read_json(os.path.join(doc_path,work+\"_profile_train.json\"))\n",
    "    \n",
    "    for work in working_area:\n",
    "        df_train[work].rename(columns=lambda x:x.replace(' ','_'), inplace=True)\n",
    "        df_val[work].rename(columns=lambda x:x.replace(' ','_'), inplace=True)\n",
    "        df_profile[work].rename(columns=lambda x:x.replace(' ','_'), inplace=True)\n",
    "    \n",
    "    # can only implement once, fix the path error with no .jpg\n",
    "    for work in working_area:\n",
    "        if work == 'fashion':\n",
    "            if fixbug == False:\n",
    "                new_image_path = (df_train['fashion']['image_path'][0:219702]+ '.jpg').tolist()+df_train['fashion']['image_path'][219702:].tolist()\n",
    "                df_train['fashion']['image_path'] = new_image_path\n",
    "                df_val['fashion']['image_path']=df_val['fashion'].image_path.apply(without_new)\n",
    "                fixbug = True\n",
    "  \n",
    "    df_class={}\n",
    "    for work in working_area:\n",
    "        df_working_class={}\n",
    "        sub_classes = df_train[work].columns.tolist()[3:]\n",
    "        for classes in sub_classes:\n",
    "            df_working_class[classes]=df_train[work][['image_path', classes]].copy()\n",
    "            df_working_class[classes].dropna(axis=0, how='any',inplace=True)\n",
    "        df_class[work]= df_working_class\n",
    "      \n",
    "    df_tag={}\n",
    "    for work in working_area:\n",
    "        df_working_tag={}\n",
    "        sub_classes = df_train[work].columns.tolist()[3:]\n",
    "        for classes in sub_classes:\n",
    "            df_working_tag[classes]=df_profile[work][classes].value_counts().keys().tolist()\n",
    "            df_working_tag[classes].sort()\n",
    "        df_tag[work]= df_working_tag\n",
    "    \n",
    "    df_cat={}\n",
    "    for work in working_area:\n",
    "        df_working_cat={}\n",
    "        sub_classes = df_train[work].columns.tolist()[3:] # get the classes for each work\n",
    "        for classes in sub_classes:\n",
    "            max_model = max(df_tag[work][classes]) + 1 # in profile json, how many tags for each classes\n",
    "            df_working_cat[classes]=to_categorical(df_class[work][classes][classes],int(max_model))\n",
    "        df_cat[work]= df_working_cat \n",
    "    \n",
    "    for work in working_area:\n",
    "        sub_classes = df_train[work].columns.tolist()[3:]\n",
    "        for classes in sub_classes:\n",
    "            for i in df_tag[work][classes]:\n",
    "                df_class[work][classes][classes+'_'+str(i)] = df_cat[work][classes][:, int(i)]\n",
    "            df_class[work][classes].drop(classes,axis=1, inplace=True)\n",
    "   \n",
    "    traing_classes={}\n",
    "    for work in working_area:\n",
    "        traing_classes_cat={}\n",
    "        sub_classes = df_train[work].columns.tolist()[3:]\n",
    "        for classes in sub_classes:\n",
    "            traing_classes_cat_temp=df_class[work][classes].columns.tolist()\n",
    "            traing_classes_cat[classes]=traing_classes_cat_temp[1:]\n",
    "        traing_classes[work]= traing_classes_cat\n",
    "    \n",
    "    return df_train,df_val,df_class,traing_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "czZaZT89DnFM"
   },
   "outputs": [],
   "source": [
    "def model_pre_adjust(num_classes=4, trainable=True):    \n",
    "    base_model = InceptionV3(input_shape=(299, 299, 3), weights='imagenet', include_top=False)\n",
    "    #base_model.summary()\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    #x = LeakyReLU(alpha=0.1)(x)\n",
    "    #predictions = Dense(5, activation='softmax')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model_test1 = Model(inputs=base_model.input, outputs = predictions)\n",
    "    if trainable:\n",
    "        for layer in model_test1.layers[:140]:\n",
    "            layer.trainable = False\n",
    "        for layer in model_test1.layers[140:]:\n",
    "            layer.trainable = True\n",
    "    else:\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False \n",
    "    return model_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4MQxMunELQR"
   },
   "outputs": [],
   "source": [
    "def data_pre_generator(work, classes):\n",
    "    #from keras.preprocessing.image import ImageDataGenerator\n",
    "    #from keras.applications.inception_v3 import preprocess_input\n",
    "    sz=299\n",
    "    batch_size = 128\n",
    "    train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                       width_shift_range=0.2,\n",
    "                                       height_shift_range=0.2,\n",
    "                                       rotation_range=20,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       horizontal_flip=True,\n",
    "                                       validation_split = 0.15)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "    train_generator = train_datagen.flow_from_dataframe(dataframe=df_class[work][classes],\n",
    "                                                        directory=repo_path, \n",
    "                                                        x_col = 'image_path',\n",
    "                                                        y_col = traing_classes[work][classes],\n",
    "                                                        class_mode = \"other\", \n",
    "                                                        subset=\"training\",\n",
    "                                                        target_size=(sz, sz),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        seed=42,\n",
    "                                                        shuffle=True\n",
    "                                                       )\n",
    "    \n",
    "    validation_generator = train_datagen.flow_from_dataframe(dataframe=df_class[work][classes],\n",
    "                                                             directory=repo_path, \n",
    "                                                             x_col = 'image_path',\n",
    "                                                             y_col = traing_classes[work][classes],\n",
    "                                                             class_mode = \"other\", \n",
    "                                                             subset=\"validation\",\n",
    "                                                             target_size=(sz, sz),\n",
    "                                                             batch_size=batch_size,\n",
    "                                                             seed=42,\n",
    "                                                             shuffle=True\n",
    "                                                            )\n",
    "    \n",
    "    test_generator=test_datagen.flow_from_dataframe(dataframe=df_val[work],\n",
    "                                                    directory=repo_path,\n",
    "                                                    x_col='image_path',\n",
    "                                                    y_col=None,\n",
    "                                                    class_mode = None,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    seed=42,\n",
    "                                                    shuffle=False,\n",
    "                                                    target_size=(sz, sz))\n",
    "    return train_generator, validation_generator,test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2l3OKwhXE-Vy"
   },
   "outputs": [],
   "source": [
    "def train_data(model_test1, train_generator, validation_generator, test_generator, work, classes):\n",
    "    #from keras.callbacks import ModelCheckpoint\n",
    "    STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\n",
    "    train_cut = train_generator.n%train_generator.batch_size\n",
    "    if train_cut!=0:\n",
    "        STEP_SIZE_TRAIN +=1\n",
    "    STEP_SIZE_VALID = validation_generator.n // validation_generator.batch_size\n",
    "    valid_cut = validation_generator.n%validation_generator.batch_size\n",
    "    if valid_cut!=0:\n",
    "        STEP_SIZE_VALID +=1\n",
    "    STEP_SIZE_TEST = test_generator.n // test_generator.batch_size\n",
    "    test_cut = test_generator.n%test_generator.batch_size\n",
    "    if test_cut !=0:\n",
    "        STEP_SIZE_TEST +=1\n",
    "    model_test1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    filepath=doc_path +'/models/weights-best_'+work+classes+'.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    hist_test_classes = model_test1.fit_generator(generator = train_generator,\n",
    "                                                  steps_per_epoch = STEP_SIZE_TRAIN,\n",
    "                                                  validation_data = validation_generator,\n",
    "                                                  validation_steps = STEP_SIZE_VALID,\n",
    "                                                  epochs = 10\n",
    "                                                 )\n",
    "    model_test1.save(doc_path +'/models/clasif10_model1_'+work+classes+'.h5')\n",
    "    evaluate_test_classes=model_test1.evaluate_generator(generator=validation_generator,\n",
    "                                                         steps=STEP_SIZE_VALID\n",
    "                                                        )\n",
    "    print('evaluation accuracy for '+work+'_'+classes +'='+ str(evaluate_test_classes[1]))\n",
    "    test_generator.reset()\n",
    "    pred_test_classes=model_test1.predict_generator(test_generator,\n",
    "                                                    steps=STEP_SIZE_TEST,\n",
    "                                                    verbose=1\n",
    "                                                   )\n",
    "    del model_test1\n",
    "    return hist_test_classes, evaluate_test_classes, pred_test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lbHfDIY8HgIr"
   },
   "outputs": [],
   "source": [
    "def submit_data_prepare(pred_test):\n",
    "    df_base = df_val[work][['itemid']].copy()\n",
    "    df_base_classes=pd.DataFrame()\n",
    "    df_base['itemid']=df_base.itemid.apply(str)\n",
    "    df_base['itemid']=df_base.itemid.apply((lambda x: is_class(x, classes)))\n",
    "    #tagging\n",
    "    print(pred_test)\n",
    "    predicted_indices=np.argsort((-1)*pred_test,axis=1)\n",
    "    predicted_top=np.delete(predicted_indices, np.s_[2:], axis=1)\n",
    "    #df_base['tagging']=predicted_top # 2D array ? \n",
    "    predicted_top=predicted_top.astype(str)\n",
    "    predicted_top_object=np.array(list(predicted_top), dtype=np.object)\n",
    "    predicted_top_series = pd.Series(list(predicted_top_object))\n",
    "    predicted_top_series=predicted_top_series.apply(strnumcombin)\n",
    "    df_base['tagging']=predicted_top_series\n",
    "    df_base.rename(columns={'itemid':'id'}, inplace=True)\n",
    "    df_base_classes=df_base_classes.append(df_base, ignore_index=True)\n",
    "    \n",
    "    return df_base_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gT8yIsPMElVr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 139467 images.\n",
      "Found 24611 images.\n",
      "Found 30135 images.\n",
      "Epoch 1/10\n",
      "1090/1090 [==============================] - 3873s 4s/step - loss: 1.2243 - acc: 0.6182 - val_loss: 1.4210 - val_acc: 0.5461\n",
      "Epoch 2/10\n",
      "1090/1090 [==============================] - 3834s 4s/step - loss: 1.0784 - acc: 0.6575 - val_loss: 1.4019 - val_acc: 0.5416\n",
      "Epoch 3/10\n",
      "1090/1090 [==============================] - 3812s 3s/step - loss: 1.0194 - acc: 0.6737 - val_loss: 1.5244 - val_acc: 0.5210\n",
      "Epoch 4/10\n",
      "1090/1090 [==============================] - 3785s 3s/step - loss: 0.9721 - acc: 0.6875 - val_loss: 1.6689 - val_acc: 0.5082\n",
      "Epoch 5/10\n",
      "1090/1090 [==============================] - 3790s 3s/step - loss: 0.9311 - acc: 0.6992 - val_loss: 1.7116 - val_acc: 0.5022\n",
      "Epoch 6/10\n",
      "1090/1090 [==============================] - 3793s 3s/step - loss: 0.8932 - acc: 0.7091 - val_loss: 1.5901 - val_acc: 0.5253\n",
      "Epoch 7/10\n",
      "1090/1090 [==============================] - 3782s 3s/step - loss: 0.8579 - acc: 0.7193 - val_loss: 1.9682 - val_acc: 0.4651\n",
      "Epoch 8/10\n",
      "1090/1090 [==============================] - 3786s 3s/step - loss: 0.8225 - acc: 0.7292 - val_loss: 1.5431 - val_acc: 0.5183\n",
      "Epoch 9/10\n",
      "1090/1090 [==============================] - 3779s 3s/step - loss: 0.7889 - acc: 0.7382 - val_loss: 1.6576 - val_acc: 0.5018\n",
      "Epoch 10/10\n",
      "1090/1090 [==============================] - 3780s 3s/step - loss: 0.7550 - acc: 0.7476 - val_loss: 1.5712 - val_acc: 0.5401\n",
      "evaluation accuracy for fashion_Pattern=0.5393116899008171\n",
      "236/236 [==============================] - 348s 1s/step\n",
      "{'Pattern': array([[1.0399550e-06, 1.1068215e-06, 1.7937667e-04, ..., 1.0529766e-06,\n",
      "        1.6105542e-03, 2.3815920e-07],\n",
      "       [7.2384267e-07, 8.8635524e-08, 3.6597844e-06, ..., 6.8439054e-05,\n",
      "        9.5529659e-03, 9.8664600e-07],\n",
      "       [3.6732999e-05, 5.6826195e-04, 2.8950849e-03, ..., 2.5323399e-03,\n",
      "        1.1996396e-01, 7.9283869e-04],\n",
      "       ...,\n",
      "       [4.6430752e-03, 1.1784676e-01, 5.9338640e-02, ..., 7.3858970e-03,\n",
      "        9.1271996e-02, 6.3221306e-03],\n",
      "       [1.6899549e-03, 1.0584796e-02, 3.5367332e-02, ..., 4.4279881e-03,\n",
      "        3.5331283e-02, 1.1382811e-02],\n",
      "       [2.2823412e-03, 1.1002505e-02, 4.1345051e-01, ..., 3.6023741e-04,\n",
      "        6.0364153e-02, 2.2787960e-02]], dtype=float32)}\n",
      "[[1.0399550e-06 1.1068215e-06 1.7937667e-04 ... 1.0529766e-06\n",
      "  1.6105542e-03 2.3815920e-07]\n",
      " [7.2384267e-07 8.8635524e-08 3.6597844e-06 ... 6.8439054e-05\n",
      "  9.5529659e-03 9.8664600e-07]\n",
      " [3.6732999e-05 5.6826195e-04 2.8950849e-03 ... 2.5323399e-03\n",
      "  1.1996396e-01 7.9283869e-04]\n",
      " ...\n",
      " [4.6430752e-03 1.1784676e-01 5.9338640e-02 ... 7.3858970e-03\n",
      "  9.1271996e-02 6.3221306e-03]\n",
      " [1.6899549e-03 1.0584796e-02 3.5367332e-02 ... 4.4279881e-03\n",
      "  3.5331283e-02 1.1382811e-02]\n",
      " [2.2823412e-03 1.1002505e-02 4.1345051e-01 ... 3.6023741e-04\n",
      "  6.0364153e-02 2.2787960e-02]]\n",
      "Pattern\n",
      "[[1.0399550e-06 1.1068215e-06 1.7937667e-04 ... 1.0529766e-06\n",
      "  1.6105542e-03 2.3815920e-07]\n",
      " [7.2384267e-07 8.8635524e-08 3.6597844e-06 ... 6.8439054e-05\n",
      "  9.5529659e-03 9.8664600e-07]\n",
      " [3.6732999e-05 5.6826195e-04 2.8950849e-03 ... 2.5323399e-03\n",
      "  1.1996396e-01 7.9283869e-04]\n",
      " ...\n",
      " [4.6430752e-03 1.1784676e-01 5.9338640e-02 ... 7.3858970e-03\n",
      "  9.1271996e-02 6.3221306e-03]\n",
      " [1.6899549e-03 1.0584796e-02 3.5367332e-02 ... 4.4279881e-03\n",
      "  3.5331283e-02 1.1382811e-02]\n",
      " [2.2823412e-03 1.1002505e-02 4.1345051e-01 ... 3.6023741e-04\n",
      "  6.0364153e-02 2.2787960e-02]]\n",
      "Found 96593 images.\n",
      "Found 17045 images.\n",
      "Found 30135 images.\n",
      "Epoch 1/10\n",
      "755/755 [==============================] - 2657s 4s/step - loss: 0.9792 - acc: 0.7004 - val_loss: 0.9487 - val_acc: 0.7075\n",
      "Epoch 2/10\n",
      "755/755 [==============================] - 2581s 3s/step - loss: 0.7943 - acc: 0.7588 - val_loss: 0.8833 - val_acc: 0.7267\n",
      "Epoch 3/10\n",
      "755/755 [==============================] - 2610s 3s/step - loss: 0.7256 - acc: 0.7790 - val_loss: 0.9333 - val_acc: 0.7109\n",
      "Epoch 4/10\n",
      "755/755 [==============================] - 2617s 3s/step - loss: 0.6777 - acc: 0.7933 - val_loss: 0.8904 - val_acc: 0.7273\n",
      "Epoch 5/10\n",
      "755/755 [==============================] - 2601s 3s/step - loss: 0.6360 - acc: 0.8077 - val_loss: 0.9067 - val_acc: 0.7292\n",
      "Epoch 6/10\n",
      "755/755 [==============================] - 2634s 3s/step - loss: 0.5979 - acc: 0.8171 - val_loss: 0.8448 - val_acc: 0.7446\n",
      "Epoch 7/10\n",
      "755/755 [==============================] - 2599s 3s/step - loss: 0.5660 - acc: 0.8268 - val_loss: 0.8161 - val_acc: 0.7573\n",
      "Epoch 8/10\n",
      "755/755 [==============================] - 2580s 3s/step - loss: 0.5360 - acc: 0.8335 - val_loss: 0.8619 - val_acc: 0.7399\n",
      "Epoch 9/10\n",
      "755/755 [==============================] - 2578s 3s/step - loss: 0.5066 - acc: 0.8443 - val_loss: 0.8818 - val_acc: 0.7368\n",
      "Epoch 10/10\n",
      "755/755 [==============================] - 2582s 3s/step - loss: 0.4789 - acc: 0.8509 - val_loss: 0.9125 - val_acc: 0.7370\n",
      "evaluation accuracy for fashion_Collar_Type=0.7333528894313657\n",
      "236/236 [==============================] - 346s 1s/step\n",
      "{'Pattern': array([[1.0399550e-06, 1.1068215e-06, 1.7937667e-04, ..., 1.0529766e-06,\n",
      "        1.6105542e-03, 2.3815920e-07],\n",
      "       [7.2384267e-07, 8.8635524e-08, 3.6597844e-06, ..., 6.8439054e-05,\n",
      "        9.5529659e-03, 9.8664600e-07],\n",
      "       [3.6732999e-05, 5.6826195e-04, 2.8950849e-03, ..., 2.5323399e-03,\n",
      "        1.1996396e-01, 7.9283869e-04],\n",
      "       ...,\n",
      "       [4.6430752e-03, 1.1784676e-01, 5.9338640e-02, ..., 7.3858970e-03,\n",
      "        9.1271996e-02, 6.3221306e-03],\n",
      "       [1.6899549e-03, 1.0584796e-02, 3.5367332e-02, ..., 4.4279881e-03,\n",
      "        3.5331283e-02, 1.1382811e-02],\n",
      "       [2.2823412e-03, 1.1002505e-02, 4.1345051e-01, ..., 3.6023741e-04,\n",
      "        6.0364153e-02, 2.2787960e-02]], dtype=float32), 'Collar_Type': array([[3.6554779e-03, 7.7001336e-05, 7.6657328e-05, ..., 2.7861559e-05,\n",
      "        1.7416896e-04, 3.8820975e-03],\n",
      "       [4.0822104e-03, 2.8412985e-02, 2.2234846e-02, ..., 2.1301128e-04,\n",
      "        1.3425757e-04, 4.1161370e-04],\n",
      "       [4.3023526e-04, 4.8918258e-03, 1.6377995e-05, ..., 7.7375276e-05,\n",
      "        1.8810254e-04, 2.8462198e-03],\n",
      "       ...,\n",
      "       [4.8374226e-03, 1.1877035e-02, 2.3419816e-02, ..., 4.5866454e-03,\n",
      "        1.5270801e-03, 1.9370342e-03],\n",
      "       [2.3510437e-04, 9.6471915e-05, 1.0102430e-04, ..., 1.7905224e-05,\n",
      "        4.0704585e-05, 1.2549701e-04],\n",
      "       [8.8609373e-03, 9.7980657e-05, 2.8264738e-04, ..., 1.4329521e-03,\n",
      "        2.0864301e-03, 1.3548651e-02]], dtype=float32)}\n",
      "[[3.6554779e-03 7.7001336e-05 7.6657328e-05 ... 2.7861559e-05\n",
      "  1.7416896e-04 3.8820975e-03]\n",
      " [4.0822104e-03 2.8412985e-02 2.2234846e-02 ... 2.1301128e-04\n",
      "  1.3425757e-04 4.1161370e-04]\n",
      " [4.3023526e-04 4.8918258e-03 1.6377995e-05 ... 7.7375276e-05\n",
      "  1.8810254e-04 2.8462198e-03]\n",
      " ...\n",
      " [4.8374226e-03 1.1877035e-02 2.3419816e-02 ... 4.5866454e-03\n",
      "  1.5270801e-03 1.9370342e-03]\n",
      " [2.3510437e-04 9.6471915e-05 1.0102430e-04 ... 1.7905224e-05\n",
      "  4.0704585e-05 1.2549701e-04]\n",
      " [8.8609373e-03 9.7980657e-05 2.8264738e-04 ... 1.4329521e-03\n",
      "  2.0864301e-03 1.3548651e-02]]\n",
      "Collar_Type\n",
      "[[3.6554779e-03 7.7001336e-05 7.6657328e-05 ... 2.7861559e-05\n",
      "  1.7416896e-04 3.8820975e-03]\n",
      " [4.0822104e-03 2.8412985e-02 2.2234846e-02 ... 2.1301128e-04\n",
      "  1.3425757e-04 4.1161370e-04]\n",
      " [4.3023526e-04 4.8918258e-03 1.6377995e-05 ... 7.7375276e-05\n",
      "  1.8810254e-04 2.8462198e-03]\n",
      " ...\n",
      " [4.8374226e-03 1.1877035e-02 2.3419816e-02 ... 4.5866454e-03\n",
      "  1.5270801e-03 1.9370342e-03]\n",
      " [2.3510437e-04 9.6471915e-05 1.0102430e-04 ... 1.7905224e-05\n",
      "  4.0704585e-05 1.2549701e-04]\n",
      " [8.8609373e-03 9.7980657e-05 2.8264738e-04 ... 1.4329521e-03\n",
      "  2.0864301e-03 1.3548651e-02]]\n",
      "Found 125022 images.\n",
      "Found 22062 images.\n",
      "Found 30135 images.\n",
      "Epoch 1/10\n",
      "977/977 [==============================] - 3480s 4s/step - loss: 1.1934 - acc: 0.5584 - val_loss: 1.1682 - val_acc: 0.5400\n",
      "Epoch 2/10\n",
      "977/977 [==============================] - 3423s 4s/step - loss: 1.0676 - acc: 0.6069 - val_loss: 1.3746 - val_acc: 0.4630\n",
      "Epoch 3/10\n",
      "977/977 [==============================] - 3432s 4s/step - loss: 1.0101 - acc: 0.6266 - val_loss: 1.1470 - val_acc: 0.5450\n",
      "Epoch 4/10\n",
      "977/977 [==============================] - 3432s 4s/step - loss: 0.9615 - acc: 0.6461 - val_loss: 1.1743 - val_acc: 0.5338\n",
      "Epoch 5/10\n",
      "977/977 [==============================] - 3437s 4s/step - loss: 0.9185 - acc: 0.6637 - val_loss: 1.1659 - val_acc: 0.5441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "977/977 [==============================] - 3438s 4s/step - loss: 0.8759 - acc: 0.6807 - val_loss: 1.1730 - val_acc: 0.5436\n",
      "Epoch 7/10\n",
      "977/977 [==============================] - 3450s 4s/step - loss: 0.8388 - acc: 0.6956 - val_loss: 1.1310 - val_acc: 0.5529\n",
      "Epoch 8/10\n",
      "976/977 [============================>.] - ETA: 3s - loss: 0.7987 - acc: 0.7089"
     ]
    }
   ],
   "source": [
    "df_submit=pd.DataFrame()\n",
    "hist_test={}\n",
    "evaluate_test={} \n",
    "pred_test={} \n",
    "\n",
    "working_area = ['fashion', 'beauty','mobile']\n",
    "\n",
    "df_train,df_val,df_class,traing_classes=data_prepare(working_area, fixbug)\n",
    "\n",
    "for work in working_area:\n",
    "    hist_test_classes={}\n",
    "    evaluate_test_classes={}\n",
    "    pred_test_classes={}\n",
    "    df_base_classes=pd.DataFrame()\n",
    "    sub_classes = df_train[work].columns.tolist()[3:]\n",
    "    for classes in sub_classes:\n",
    "        K.clear_session()\n",
    "        df_base = df_val[work][['itemid']].copy()\n",
    "        model_test1=model_pre_adjust(num_classes=len(traing_classes[work][classes]), trainable=True)\n",
    "        train_generator, validation_generator,test_generator = data_pre_generator(work, classes)\n",
    "        hist_test_classes[classes], evaluate_test_classes[classes],pred_test_classes[classes]=train_data(model_test1, train_generator, validation_generator, test_generator, work, classes)\n",
    "        df_base_classes=df_base_classes.append(submit_data_prepare(pred_test_classes[classes]),ignore_index=True)\n",
    "        df_base_classes.to_csv(doc_path +'/'+work+'_'+classes+'submit.csv')\n",
    "    hist_test[work]=hist_test_classes\n",
    "    evaluate_test[work]=evaluate_test_classes\n",
    "    pred_test[work]=pred_test_classes\n",
    "    df_submit=df_submit.append(df_base_classes, ignore_index=True)\n",
    "\n",
    "\n",
    "df_submit.to_csv(doc_path +'/submit_fashion.csv')\n",
    "df_submit.info()\n",
    "\n",
    "\n",
    "for work in working_area:\n",
    "    sub_classes = df_train[work].columns.tolist()[3:]\n",
    "    for classes in sub_classes:\n",
    "        plt.plot(hist_test[work][classes].history['loss'], color=\"#F7522F\")\n",
    "        plt.plot(hist_test[work][classes].history['val_loss'], color=\"#2F8FF7\")\n",
    "        plt.title('model loss of '+work+' '+classes)\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train','validation'], loc= 'upper left')\n",
    "        plt.savefig(doc_path +'/'+work+'_'+classes+'_loss.png')\n",
    "        \n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZhCT8cdUGL7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Transfer_learning_subdf_0_colab_tensorflow_ongoing.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
